\newpage
\section{Python Implementation of the Softmax Classifier}
\begin{minted}{python}
import load_mnist as lm
import numpy as np

class SoftmaxClassifier(object):
  def __init__(self, num_classes):
    ''' initialize model parameters
        num_classes -- number of classes
    '''
    self.num_classes = num_classes
  
  def train(self, X, y, batch_size=128, num_iterations=1000, learning_rate=.01):
    ''' train classifier for given number of iterations
        X               -- entire set of training data
        y               -- labels for training data
        batch_size      -- number of training examples per batch
        num_iterations  -- number of iterations to train for
        learning_rate   -- how much we move with the gradient at each iteration
    '''
    self.learning_rate = learning_rate
    self.W = np.zeros((X.shape[1], self.num_classes))
    self.b = np.zeros((1, self.num_classes))
    
    for iteration in range(num_iterations):
      for batch in range(0, len(X), batch_size):
        X_batch = X[batch:batch + batch_size]
        y_batch = y[batch:batch + batch_size]
        self.train_batch(X_batch, y_batch)
  
  def train_batch(self, X, y):
    ''' perform gradient descent on batch. contains:
          - forward pass
          - backpropogation
        X -- batch of training data (size batch_size * num_features)
        y -- labels for X (size 1 * batch_size)
    '''
    # forward pass
    P = np.dot(X, self.W) + self.b

    # back-prop
    E = np.exp(P)
    D = E / E.sum(axis=1).reshape((y.size, 1)) # sum over rows, shape
    R = np.arange(y.size)
    Y = np.zeros((y.size, self.num_classes))
    Y[R, y] = 1

    gradient_P = (-Y + D) / len(X)
    gradient_W = np.dot(np.transpose(X), gradient_P)
    gradient_b = gradient_P.sum(axis=0) # sum over columns

    # update
    self.W -= self.learning_rate * gradient_W
    self.b -= self.learning_rate * gradient_b
  
  def predict(self, X):
    ''' predict using best score for all training examples in X
        X -- training dataset
    '''
    return np.argmax(np.dot(X, self.W) + self.b, axis=1)
\end{minted}
