\newpage
\section{Gradient Descent}\label{ch:gradient_descent} 
%explain batches

%explain weights, that weights change after a batch props, not whole network

\subsection{Linear Classification}

Suppose our training data is represented by a matrix $X$ which has dimensions
$m \times n$, meaning there are $m$ examples, each with $n$ features. Let us
suppose that each piece of data in $X$ belongs to one of $k$ classes, and the
labels corresponding to each example are stored in a matrix $y$ which has
dimension $m \times 1$, where the $i^{th}$ entry gives the class of the
$i^{th}$ row of $X$.

Suppose that we have a matrix $W$ with dimension $n \times k$ and a vector $b$
of dimension $k \times 1$ which are both parameters of our neural network. We
can build a prediction matrix $P$ where

\[
P = XW + b.
\]

Note that $XW$ is of size $m \times k$ and $b$ is of size $1 \times k$.
In order to add these together, we need to use a technique
called broadcasting, where we turn $b$ into an $m \times k$ matrix where each
row of this new matrix corresponds to $b$. This makes $XW$ and $b$ the
same size so that they can be added to create $P$.

Each row of $P$ contains $k$ scores that represent how closely that
training example matches each class. A prediction can be made by finding the
class which corresponds to the entry of the highest score. The accuracy of
these predictions can be evaluated with respect to the class labels found in
$y$.

We will now discuss how to determine $W$ and $b$ given our training data $X$
and its labels $y$. The goal of training a linear classifier is to determine
the optimal values for these parameters.

\subsection{Loss Functions}

Suppose we have a function $l$ which takes in one training example and its
label as input and returns some sort of loss for that training example.  The
function $L$, defined below, aggregates the values of $l$ for each training
example. We call this our loss function

\[
L(p, y) = \sum_{i=1}^m l(P_i, y_i),
\]
where $P_i$ denotes the $i^{th}$ row of $P$ and $y_i$ denotes the $i^{th}$
entry of $y$.

We will be using a specific loss function called the Softmax Loss function,
which is defined by

\[
L(P) = -\frac{1}{m} \sum_{i=1}^m \log \frac{\exp{P_{iy_i}}}
                                           {\sum_{j=1}^k \exp{P_{ij}}}.
\]

By substituting $P = XW + b$, we get

\[
L(P) = -\frac{1}{m} \sum_{i=1}^m \log
        \frac{\exp{\left(XW + b\right)_{iy_i}}}
             {\sum_{j=1}^k \exp{\left(XW + b\right)_{ij}}}.
\]

\subsection{Deriving Gradient Update}
To make successful predictions, we want to minimize our loss function $L$. We
will do this using the gradient descent algorithm, which is an iterative
algorithm that updates the parameters of our linear classifier. Gradient
descent does this by finding the derivates of $L$ with respect to each
parameter, and updating the parameters using that information by following the
direction of the gradient. After each iteration, $W$ and $b$ will be updated so
that $L$ is smaller; we approach a local mininum of $L$ through this process.

The gradient descent algorithm is generally performed on subsets of data, which
are called batches. This reduces the size of matricies such as $W$ and $X$,
which will reduce the time and space that the algorithm takes up. The number of
training examples processed at once is called the \textit{batch size} and is a
hyperparameter, or a parameter that is set by the user and doesn't change with
training. For each iteration of gradient descent, we split our data into
batches and iterate through each batch.

Another hyperparameter is the \textit{learning rate}, denoted by $\eta$; it
effectively scales how much we will ``follow" the gradient. If $\eta$ is
very small, then the network will take a long time to train, but if $\eta$ is
too large, the network will have trouble finding the optimal $W$ and $b$, which
will cause the training and test accuracies to drop.

\noindent There are methods for finding optimal values for $\eta$ and other
hyperparameters, but this topic goes beyond the scope of this paper.

We will now
derive the gradient update formula for the Softmax Loss function.

\subsubsection{Deriving $\nabla P$}

For any matrix $M$, we can define $\nabla M$ to be a matrix of partial
derivatives such that $\nabla M_{ij} = \frac{\partial L}{\partial M_{ij}}.$
We first will determine $\nabla P$ as a step in determining $\nabla W$ and
$\nabla b$. Let us fix a pair of indices $u$, $v$ where $1 \leq u \leq n$ and
$1 \leq v \leq k$. First, we will differentiate with respect to $P_{uv}$.
Let us fix an $i$ where $1 \leq i \leq m$.  Let $L_i = \frac{\exp{P_{iy_i}}}
{\sum_{j=1}^k \exp{P_{ij}}}$.

Let us also define a matrix $Y$ of size $m \times k$ as $Y_{ij} = 1(y_i == j)$,
where $1(a == b)$ evaluates to 1 if $a$ is equal to $b$, and evaluates to 0
otherwise. So we have 

\begin{align*}
  \frac{\partial L}{\partial P_{uv}}
  &= \sum_{i=1}^m \frac{\partial L_i}{\partial P_{uv}}\\
  &= -\frac{1}{m} Y_{uv} + \frac{1}{m}
  \frac{\exp{P_{uv}}}{\sum_{j=1}^k \exp{P_{ij}}}.
\end{align*}

Thus $\nabla P = \frac{1}{m} \left[ -Y + \frac{\exp{P}}{\sum_{i=1}^m
    \exp{P_i}} \right]$.

\subsubsection{Deriving $\nabla W$}
Now we will derive $\nabla W$ to use in the update step of training. Recall
that $P = XW + b$, meaning $P_{ij} = \sum_{t=1}^n X_{it}W_{tj} + b_j $.
Also remember that $L_i$ is a function of $P_{uv}$ only if $u == i$, so it
follows that $\nabla P_{ij} = \frac{\partial L}{\partial P_{ij}} =
\frac{\partial L_i}{\partial P_{ij}}$. The following goes through the
derivation of $\frac{\partial L}{\partial W_{uv}}$.

\begin{align*} 
     \frac{\partial L}{\partial W_{uv}} = 
     \sum_{i=1}^m \frac{\partial L_i}{\partial W_{uv}} &= 
     \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{ij}}
       \frac{\partial P_{ij}}{\partial W_{uv}}\\
     &= \sum_{i=1}^m \sum_{j=1}^k
         \frac{\partial L_i}{\partial P_{ij}} X_{iu}\\
     &= \sum_{i=1}^m \frac{\partial L_i}{\partial P_{iv}} X_{iu}\\
     &= \sum_{i=1}^m \nabla P_{iv} X_{iu}\\
     &= \left( X^T \nabla P \right)_{uv}.
\end{align*}

Thus, we have
$$ \nabla W = X^T \nabla P. $$

\subsubsection{Deriving $\nabla b$}
We will now derive $\nabla b$ to use in the update step of training. This is
similar to the derivation for $\nabla W$.

We know that $\frac{\partial P_{ij}}{\partial b_u} = 1(j == u).$
\begin{align*} 
  \frac{\partial L}{\partial b_u} = 
  \sum_{i=1}^m \frac{\partial L_i}{\partial b_u} &= 
  \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{i, j}}
  \frac{\partial P_{ij}}{\partial b}\\
  &= \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{ij}} 1(j == u)\\
  &= \sum_{i=1}^m \frac{\partial L_i}{\partial P_{iu}} \\
  &= \sum_{i=1}^m \nabla P_{iu}.
\end{align*}

Thus $\nabla b = \sum_{i=1}^m \nabla P_i$.

\subsection{Gradient Descent Update}
Now that we have computed $\nabla W$ and $\nabla b$, we can update $W$
and $b$ by the following:

\begin{align*}
W &= W - \eta \nabla W,\\
b &= b - \eta \nabla b, 
\end{align*}

At this point, we have enough information to create a Softmax Classifier, which
is a zero-layer neural net. Recall that when we refer to the
number of layers a neural net has, we refer to the number of hidden layers.


\subsection{Deriving $\nabla X$}
We will now derive $\nabla X$. This is not necesary for the Softmax Classifier,
but will be useful for the one-layer neural network derivation in a later
chapter.

\begin{align*}
  \frac{\partial l}{\partial x_{uv}}
  =\sum_{i=1}^m \frac{\partial l_i}{x_{uv}}\\
  &= \sum_{i=1}^m \sum_{j=1}^k \frac{\partial l_i}{\partial p_{ij}}
      \frac{\partial p_{ij}}{\partial x_{uv}}\\
  &= \sum_{i=1}^m \sum_{j=1}^k \frac{\partial l_i}{\partial p_{ij}}
      1(i == u) w_{vj}\\
  &= \sum_{j=1}^k \sum_{i=1}^m \frac{\partial l_i}{\partial p_{ij}}
      1(i == u) w_{vj}\\
  &= \sum_{j=1}^k \frac{\partial l_u}{\partial p_{uj}} w_{vj}\\
  &= \sum_{j=1}^k \nabla p_{uj} w_{vj}\\
  &= \left( \nabla p w^t \right) _{uv}.
\end{align*}

defining $\nabla x_{ij} = \frac{\partial l}{\partial x_{ij}}$, we now have

$$ \nabla x = \nabla p w^t. $$
