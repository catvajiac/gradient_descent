\newpage
\section{Gradient Descent}\label{ch:gradient_descent} 
%explain batches

%explain weights, that weights change after a batch props, not whole network

\subsection{Linear Classification}

Suppose our training data is represented by a matrix $X$ which has dimensions
$m \times n$, meaning there are $m$ examples, each with $n$ features. Let us
suppose that each piece of data in $X$ belongs to one of $k$ classes, and the
labels corresponding to each example are stored in a matrix $y$ which has
dimension $m \times k$, where each row of $y$ contains a one in the column
corresponding to the class which that example corresponds to, and zeros
everywhere else.

Suppose that we have a matrix $W$ with dimension $n \times k$ and a vector $b$
of dimension $k \times 1$. We can build a prediction matrix $P$ where

\[
P = XW + b.
\]

Note that we must broadcast $b$ to an $m \times k$ matrix to compute this sum.
Each row of $P$ contains $k$ scores, which each represent how closely that
training example matches each class. A prediction can be made by predicting the
class which corresponds to the entry of the highest score. The accuracy of
these predictions can be evaluated with respect to the class labels found in
$y$.

We will now discuss how to determine $W$ and $b$ given our training data $X$
and its labels $y$. The goal of training a softmax classifier is to determine
the optimal values for these parameters.

\subsection{Loss Functions}

Suppose we have a function $l$ which takes in one training example and its
label data as input and returns some sort of loss for that training example.
The function $L$, defined below, then aggregates the values of $l$ for each
training example. We call this our loss function

\[
L(p, y) = \sum_{i=1}^m l(P_i, y_i),
\]

where $P_i$ denotes the $ith$ row of $P$ and $y_i$ denotes the $ith$ row of
$y$.

We will be looking at the Softmax Loss function, which is defined by

\[
L(P) = -\frac{1}{m} \sum_{i=1}^m \log \frac{\exp{P_{i, y_i}}}
                                           {\sum_{j=1}^k \exp{P_{i, j}}}.
\]

Note that $P$ is dependent on $W$ and $b$, so this can be rewritten as

\[
L(P) = -\frac{1}{m} \sum_{i=1}^m \log
        \frac{\exp{\left(XW\right)_{i, y_i} + b_{y_i}}}
             {\sum_{j=1}^k \exp{\left(XW\right)_{i, j} + b_j}}.
\]

\subsection{Deriving Gradient Update}
We will now work to minimize $L$ by applying gradient descent. The following
will derive the gradient update formula for the Softmax Loss function.

\subsubsection{Deriving $\nabla P$}
We first will determine $\nabla P$ as a step in determining $\nabla W$ and
$\nabla b$. Let us fix a pair of indices $u$, $v$ where $1 \leq u \leq n$ and
$1 \leq v \leq k$. First, we will differentiate with respect to $P_{u, v}$.
Let us fix an $i$ where $1 \leq i \leq m$.  Let $L_i = \frac{\exp{P_{i,
y_i}}}{\sum_{j=1}^k \exp{P_{i, j}}}$.

%Note that \frac{\partial L_u}{\partial P_{u, v} = 0 if $u \neq i$.

Let us also define a matrix $Y$ of size $m \times k$ as $Y_{i, j} = 1(y_i =
j)$. So we have $\frac{\partial L_i}{\partial P_{i,v}} = \sum_{i=1}^m
\frac{\exp{P_{i,v}}}{\sum_{i=1}^k \exp{P_{i,j}}}$.

This makes
\begin{align*}
  \frac{\partial L}{\partial P_{i,v}}
  &= \sum_{i=1}^m \frac{\partial L_i}{\partial P_{i,v}}\\
  &= -\frac{1}{m} Y_{i,v} + \frac{1}{m}
  \frac{\exp{P_{i,v}}}{\sum_{j=1}^k \exp{P_{i,j}}}
\end{align*}
  
since $Y_{i,j}$ only depends on $P_{i,w}$.

Thus $\nabla P_{i,v} = \frac{\partial L}{\partial P_{i,v}} \forall i,v.$

So we have $\partial P_{i,v} = \frac{1}{m} \left[ -Y +
\frac{\exp{P}}{\sum_{i=1}^m \exp{P_i}} \right]$.

\subsubsection{Deriving $\nabla W$}
Now we will derive $\nabla W$ to use in the update step of training. Recall
that $P = XW + b$, meaning $P_{i, j} = \sum_{t=1}^n X_{i, t}W_{t, j} + b_j $.
Also remember that $L_i$ is a function of $P_{u, v}$ only if $u = i$, so it
follows that $\nabla P_{i, j} = \frac{\partial L}{\partial P_{i, j}} =
\frac{\partial L_i}{\partial P_{i, j}}$. The following goes through the
derivation of $\frac{\partial L}{\partial W_{uv}}$.

\begin{align*} 
     \frac{\partial L}{\partial W_{u, v}} = 
     \sum_{i=1}^m \frac{\partial L_i}{\partial W_{u, v}} &= 
     \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{i, j}}
       \frac{\partial P_{i, j}}{\partial W_{u, v}}\\
     &= \sum_{i=1}^m \sum_{j=1}^k
         \frac{\partial L_i}{\partial P_{i, j}} X_{i, u}\\
     &= \sum_{i=1}^m \frac{\partial L_i}{\partial P_{i, v}} X_{i, u}\\
     &= \sum_{i=1}^m \nabla P_{i, v} X_{i. u}\\
     &= \left( X^T \nabla P \right)_{u, v}.
\end{align*}

Thus, we have
$$ \nabla W = X^T \nabla P. $$

\subsubsection{Deriving $\nabla b$}
We will now derive $\nabla b$ to use in the update step of training. This is
similar to the derivation for $\nabla W$.

We know that $\frac{\partial P_{i,j}}{\partial b_u} = 1(j == u).$
\begin{align*} 
  \frac{\partial L}{\partial b_u} = 
  \sum_{i=1}^m \frac{\partial L_i}{\partial b_u} &= 
  \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{i, j}}
  \frac{\partial P_{i, j}}{\partial b}\\
  &= \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{i, j}} 1(j == u)\\
  &= \sum_{i=1}^m \frac{\partial L_i}{\partial P_{i,u}} \\
  &= \sum_{i=1}^m P_{i,u}.
\end{align*}

Thus $\nabla b = \sum_{i=1}^m P_i$.

\subsection{Gradient Descent Update}
Now that we have computer $\nabla W$ and $\nabla b$, we can update $W$
and $b$ by the following:

\begin{align*}
W &= W - \eta \nabla W,\\
b &= b - \eta \nabla b, 
\end{align*}

where $\eta$ is a hyperparameter called the learning rate; it effectively
scales how much parameters will follow the gradient. If $\eta$ is very small,
then the network will take a long time to train, but if $\eta$ is too large,
the network will have trouble finding the optimal $W$ and $b$, which will cause
the training and test accuracies to drop. There are methods for finding optimal
values for $\eta$, but this topic goes beyond the scope of this paper.

At this point, we have enough information to create a Softmax Classifier, which is a
zero-layer neural net. (Note: recall that when we refer to the number of layers
a neural net has, we refer to the number of hidden layers)

\subsection{Classifying Non-Linear Data}
The Softmax Classifier is good at classifying linearly separable data. However,
many classification problems involve data that isn't linearly separable. In
order to remedy this, we must add a non-linearity to our neural network
architecture. There are many different ways in which this can be done, but the
current most popular function used to provide non-linearities in neural network
is called the REctified Linear Unit (RELU) function, and is defined as follows:

\[ relu(x) = \begin{cases} 
      0 & x\leq 0 \\
      x & x > 0 
   \end{cases}
\]

By adding RELU to our architecture, we will create a one-layer neural network,
which involves the following:
\begin{enumerate}
\item parameters $W^{(1)}$ and $B^{(1)}$
\item parameters $W^{(2)}$ and $B^{(2)}$ \item a non-linear function (the
$relu$ function in our case), denoted $\phi$, which maps $\mathbb{R}
\longrightarrow \mathbb{R}$.
\end{enumerate}

Our neural network now has more parameters, and so the prediction step is a
little more complicated. Data gets mapped to predictions by the following
process:
\begin{enumerate}
\item $A = XW^{(1)} + b^{(1)}$
\item $Z = \phi(A)$
\item $P + ZW^{(2)} + b^{(2)}$
\end{enumerate}

Once again, we must use a loss function to optimize our parameters. Let us use
the Softmax Loss function as we did before. (Note: current research shows that
other loss functions produce better results, but those functions tend to be
more complicated -> are more computationally expensive to train. Everything is
a tradeoff).

We will need $\phi\prime$ to calculate our update formulas, but we note that
$\phi$ is not differentable at $x=0$. In place of $\phi\prime(x)$, we will use
$1(x == 0)$. (Note: why can we do this)

\subsubsection{Deriving $\nabla W^{(2)}$ and $\nabla b^{(2)}$}
Since the output layer, or last layer of our new architecture is identical to a
Softmax classifier, just with $Z$ in place of $X$, we can define $\nabla
W^{(2)}$ and $\nabla b^{(2)}$ as

$$ \nabla W^{(2)}  = Z^T P. $$
$$ \nabla b^{(2)}  = \sum_{i=1}^m P_i. $$

\subsubsection{Deriving $\nabla X$}
\begin{align*}
  \frac{\partial L}{\partial X_{uv}}
    =\sum_{i=1}^m \frac{\partial L_i}{X_{uv}}\\
  &= \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{ij}}
      \frac{\partial P_{ij}}{\partial X_{uv}}\\
  &= \sum_{i=1}^m \sum_{j=1}^k \frac{\partial L_i}{\partial P_{ij}}
      1(i == u) W_{vj}\\
  &= \sum_{j=1}^k \sum_{i=1}^m \frac{\partial L_i}{\partial P_{ij}}
      1(i == u) W_{vj}\\
  &= \sum_{j=1}^k \frac{\partial L_u}{\partial P_{uj}} W_{vj}\\
  &= \sum_{j=1}^k \nabla P_{uj} W_{vj}\\
  &= \left( \nabla P W^T \right) _{uv}.
\end{align*}

Defining $\nabla X_{ij} = \frac{\partial L}{\partial X_{ij}}$, we now have

$$ \nabla X = \nabla P W^T. $$

% rewrite, X can be replaced iwth Z?
Deriving $\nabla Z$ would be done in exactly the same way. If we define
$\nabla Z_{uv} = \frac{\partial L}{\partial Z_{uv}}$, we now have

$$ \nabla Z = \nabla P W^{(2)}. $$

\subsubsection{Deriving $\nabla A$, $\nabla W^{(1)}$, $\nabla b^{(1)}$}
Recall that $Z = \phi(A).$

In order to calculate $\nabla W^{(1)}$ and $\nabla b^{(1)}$, we will first
calculate $\nabla A$. To do this, we will recall that $Z_{uv} = \phi(A_{uv})$
and that $L_i$ is a function of $Z_{uv}$ only if $i == u$ for any $i, u, \in
[0, m]$. We see that

\begin{align*}
  \frac{\partial L}{\partial A_{uv}}
  &= \sum_{i=1}^m \frac{\partial L_i}{\partial A_{uv}}\\
  &= \sum_{i=1}^m \frac{\partial L_i}{\partial Z_{uv}} \frac{\partial Z_{uv}}{\partial A_{uv}}
  &= \frac{\partial L_u}{Z_{uv}} 1(A_{uv} > 0)
  &= \frac{\partial L}{Z_{uv}} 1(A_{uv} > 0)
  &= \nabla Z_{uv} 1(A_{uv} > 0).
\end{align*}

So we have
$$ \nabla A = \nabla Z \dot (A > 0), $$

where $a \dot b$ indicates the element-wise multiplication of matricies $a$ and $b$.

We can define matricies $W^{(1)}$ and $b^{(1)}$ as we did with $W^{(2)}$ and
$W^{(2)}$ earlier. Since these matricies have the same relationship with $A$ as
$W$ and $b$ did with $P$ in the softmax classifier, we find that
$$ \nabla W^{(1)} = X \nabla A, $$
$$ \nabla b^{(1)} = \sum_{i=1}^m \nabla A_i.$$

\subsubsection{Update formulas}
Now that we have calculated the gradients for all of our parameters, we can
define their update formulas as follows:

\begin{align*}
W^{(1)} &= W^{(1)} - \eta \nabla W^{(1)},\\
b^{(1)} &= b^{(1)} - \eta \nabla b^{(1)},\\
W^{(2)} &= W^{(2)} - \eta \nabla W^{(2)},\\
b^{(2)} &= b^{(2)} - \eta \nabla b^{(2)}.
\end{align*}

\subsection{Arbitrary-Layer Neural Net}
We will now dicuss a fully connected neural network with an arbitary number of
hidden layers. Let $h$ be a non-negative integer that denotes the number of
hidden layers in our network. We will have $h+1$ matricies $W^{(1)}, W^{(2)},
..., W^{(h+1)}$ and vectors $b^{(1)}, b^{(2)}, ..., b^{(h+1)}$, as well as
$h+2$ positive integers $t_0, t_1, ..., t_{h+1}$, which each represent the number
of nodes in a given layer. As before, we have $k$ classes, $m$ training
examples, and $n$ features for each training example. For any arbitrary layer
$j$, the size of $W^{(j)}$ will be $t_{j-1} \times t_j$ and the size of
$b^{(j)}$ will be $1 \times t_j$. Since $t_0$ represents the number of nodes in
the input layer, it must be true that $t_0 = n$. Similarly, since $t_{h+1}$
represents the number of nodes in the output layer, it must be true that
$t_{h+1} = k$.

There are three steps to updating parameters in this arbitrary-layer network:
\begin{enumerate}
\item forward propagation
\item backward propagation
\item update parameters
\end{enumerate}

Let us go into each of these steps in more detail.

\subsubsection{Forward Propagation}
For any layer $j$ in the network, we transform an input, denoted $Z^{(j-1)}$,
of size $m \times t_{(j-1)}$ to an output, denoted $Z^{(j)}$, of size $m \times
t_j$. This is done through the following steps:

$$ A^{(j)} = Z^{(j-1)} W^{(j)} + b^{(j)}, $$
$$ Z^{(j)} = \phi\left( A^{(j)} \right)$$.

For the input layer, $Z^{(0)} = X$, since our training data must be the input
for the first layer of the network. The output layer doesn't apply the
nonlinearity $\phi$ as with the other hidden layers; instead we have
$$ P = Z^{(h)} = W^{(h+1)} + b^{(h+1)}. $$


\subsubsection{Backward Propagation}
We can define matricies $\nabla W^{(j)}$ and $\nabla b^{(j)}$ exactly as
before. The output layer of the neural network is identical to a softmax
classifier, so the formulas for $\nabla P$, $\nabla W^{(h+1)}$, and $\nabla
b^{(h+1)}$ will be identical to the ones in that section. Then, the matrix
$\nabla Z^{(h)}$ will be back-propagated to the previous layer. Let us suppress
the indices for simplicity. For each hidden layer, we recieve an input $Z_{in}$
and have parameters $W$ and $b$. We compute $A$ by

$$ A = Z_{in} W + b, $$

similarly to the one-layer network above. We then output a matrix $Z_{out}$, given by

$$ Z_{out} = \phi(A).$$

During back-propagation, each layer will recieve a matrix $\nabla Z_{out}$ from
the subsequent layer. We then compute

$$ \nabla A = \nabla Z_{out} \dot (A > 0), $$

as we did for the one-layer network. Recall that in this case, $a \dot b$
symbolizes element-by-element multiplication of two matricies $a$ and $b$.
Just as in the case for a single-layer network, we have

$$ \nabla W = Z_{in} \nabla A, $$
$$ \nabla b = \sum_{i=1}^m \nabla A_i. $$

If we are not at the input layer, we would then backpropagate $\nabla Z_{out}$
to the previous layer, where $\nabla Z_{out}$ is defined as

$$ \nabla Z_{out} = \nabla A W. $$

\subsubsection{Update Parameters}
As before, we update our parameters with the following equations:

\begin{align*}
W^{(j)} &= W^{(j)} - \eta \nabla W^{(j)},\\
b^{(j)} &= b^{(j)} - \eta \nabla b^{(j)}
\end{align*}

for each layer $j$ in the network.
