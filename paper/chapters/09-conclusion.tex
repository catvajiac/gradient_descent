\newpage
\section{Conclusion}

In this paper, we've dicussed the theory involved in designing a neural
network, particularly by deriving the equations behind the Gradient Descent
algorithm.
We have also discussed the application of neural networks to the MNIST
dataset, image classification problem, using Python.


\subsection{Future Directions}
If time permitted, there are many other topics within neural networks that
could be explored.

Naturally, there are many techniques that are used to increase the final accuracy
of a neural network. Particularly in image processing, using convolutional
layers in a neural network is quite popular. Convolutional Neural Networks tend
to be easier to train and have fewer parameters than fully-connected networks
of the same size. 
Another technique is called regularization, which attempts to prevent
the network from overfitting data during training.
There's also quite a bit of study behind the choosing of hyperparameters and
weight initialization.

Neural networks are, by design, very computationally expensive. For this
reason, many optimizations, including employing GPUs, have been developed.
Another way to reduce the training time of a neural network is to reduce the
numerical precision, or the amount of bits that store each element in the
matricies involved in training.
