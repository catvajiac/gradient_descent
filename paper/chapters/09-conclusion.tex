\newpage
\section{Conclusion}

In this paper, we've dicussed the theory involved in designing a neural
network, particularly by deriving the equations behind the Gradient Descent
algorithm.
We have also discussed the implementation of neural networks to the MNIST
dataset, image classification problem, using Python.


\subsection{Future Directions}
If time permitted, there are many other topics within neural networks that
could be explored.

Naturally, there's many techniques that are used to increase the final accuracy
of a neural network. Particularly in image processing, using convolutional
layers in a neural network is quite popular. Convolutional Neural Networks tend
to be easier to train and have fewer parameters than fully-connected networks
of the same size. 
Another neural network features is called dropout, which attempts to prevent
the network from overfitting data during training.
There's also quite a bit of study behind the choosing of hyperparameters and
weight initialization.

Another important topic beyond the scope of this paper is data preproccesing. 
The goal of data preprocessing is to reduce noise or variation in the data so
that it is easier to classify.

Neural networks are, by design, very computationally expensive. For this
reason, many optimizations, including GPU implementations, have been developed.
Another way to reduce the training time of a neural network is to reduce the
numerical precision, or the amount of bits that store each element in the
matricies involved in training.
